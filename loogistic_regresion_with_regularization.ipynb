{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder_X=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(train,test):\n",
    "    all_data=pd.concat([train.drop('Survived',1),test])\n",
    "    \n",
    "    #Feature Title, Master, miss and other titles give us the indication that the person was young, other titles looks that the person was relativly old\n",
    "    all_data['Title']=all_data['Name'].map(lambda complete_name: complete_name.split(',')[1].split('.')[0].strip())\n",
    "    titles=set(all_data['Title'])\n",
    "    temp_data=all_data[:len(train)]\n",
    "    titles_mapping=dict(zip(titles,range(0,len(titles)+1)))\n",
    "    temp_data['Numeric_title']=temp_data.apply(lambda passanger:titles_mapping[passanger.Title],axis=1).astype(int)\n",
    "    all_data['Numeric_title']=all_data.apply(lambda passanger:titles_mapping[passanger.Title],axis=1).astype(int)\n",
    "    #Fill unkonw embarcation with the mode of the embarked zone of the train data\n",
    "    all_data.loc[all_data.Embarked.isnull(),'Embarked']=train.Embarked.mode()[0]\n",
    "    #check why next line doesnt return int, instead return the letter\n",
    "    #all_data['Embarket']=all_data['Embarked'].map({'C': 0, 'S': 1, 'Q': 2}).astype(int)\n",
    "    all_data['Embarked']=labelencoder_X.fit_transform(all_data['Embarked'])\n",
    "    embarket_place=sorted(all_data['Embarked'].unique(), key=lambda x: x)\n",
    "    #Now let's fill missing age with the average of each title class but only using the train\n",
    "    average_age_title={}\n",
    "    for i in range(0,len(titles_mapping)):\n",
    "        average_age_title[i]=np.median(train.loc[temp_data.Numeric_title==i,('Age')].dropna())\n",
    "    all_data.loc[all_data.Age.isnull(),('Age')]=all_data[all_data.Age.isnull()].apply(lambda element: average_age_title.get(element.Numeric_title),axis=1)\n",
    "    #From histogram people up to 10 have better prob to survive, for people with more than 63 is the opposite\n",
    "    all_data['Young']=all_data.apply(lambda element: int(element.Age<=10),axis=1)\n",
    "    all_data['Late_adulthood']=all_data.apply(lambda element: int(element.Age>=63),axis=1)\n",
    "    #family size, non_including his/her-self\n",
    "    all_data['Family_size']=all_data['SibSp']+all_data['Parch']\n",
    "    #If the person is not-alone has better chances to survive\n",
    "    all_data['Alone']=all_data.apply(lambda element: int((element.SibSp+element.Parch)==0),axis=1)\n",
    "    #young+alone seem like has less probablity to live, but maybe is not so important since in the train only 1 is young and alone\n",
    "    all_data['Young_alone']=all_data.apply(lambda element: (element.Young==1)and(element.Alone==1),axis=1).astype(int)\n",
    "    #Also bigger families lower prob to survive :(\n",
    "    all_data['Big_family']=all_data.apply(lambda element: (element.SibSp>2)or(element.Parch>3),axis=1).astype(int)\n",
    "    #Obviuslly sex has a big impact, female has better prob to survive, there are sklearn ways to do this but pandas is really nice\n",
    "    all_data['Sex']=all_data['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "    #There is some correlation between fare and embarket place so let's use the mean/median (need to pay attention to the skwekness) for each embarket place\n",
    "    # however let;s do this in future versions\n",
    "    \n",
    "    average_fare_class=np.zeros([3,3])\n",
    "    #there are 3 classes so let's use it\n",
    "    for i in range(1,4):\n",
    "        for j in range(1,4):\n",
    "            average_fare_class[i-1][j-1]=(np.nanmedian((train.Pclass==i)&(train.Embarked==j)))\n",
    "    all_data.loc[all_data.Fare.isnull(),('Fare')]=all_data[all_data.Fare.isnull()].apply(lambda element: average_fare_class[element.Pclass-1][element.Embarked-1],axis=1)\n",
    "    \n",
    "    ##Number of roomates, roomates probably have the same fait, however I don't sure if this is OK for persons with a lot of cabins\n",
    "    roomates=train.groupby(['Cabin'])['Survived'].sum()\n",
    "    roomates_dic=dict(zip(roomates.index,roomates.values))\n",
    "    all_data['Roomates']=all_data.apply(lambda element: (roomates_dic.get(element.Cabin)-1 if (roomates_dic.get(element.Cabin)>0) else roomates_dic.get(element.Cabin) ) if (str(element.Cabin)!='nan') else 0,axis=1)\n",
    "    #need remove nan from cabin from the test then\n",
    "    all_data.loc[all_data.Roomates.isnull(),('Roomates')]=0\n",
    "    ##people with the same surename, however there are common surename so is not 100% perfect and can affect the prediction\n",
    "    all_data['Surname']=all_data['Name'].map(lambda element: element.split(',')[0].strip())\n",
    "    surename_list=sorted(all_data['Surname'].unique(), key=lambda x: x)\n",
    "    all_data['Surname']=labelencoder_X.fit_transform(all_data['Surname'])\n",
    "\n",
    "    return all_data[:len(train)],all_data[len(train):]\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=data_prep(data_set,test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=train.drop(['Ticket','Cabin','Name','Title','Surname'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survived_train=data_set['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standar_scaler=StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=standar_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_lasso={'C':np.logspace(-4,4,100),'penalty':['l1']}#these are c \n",
    "parameters_ridge={'alpha':np.logspace(-4,4,100)}# these are alpha\n",
    "l1_ratio=np.logspace(-2,0,10)\n",
    "parameters_elastic={'penalty':['elasticnet'],'alpha':np.logspace(-4,4,100),'l1_ratio':l1_ratio,'loss':['log']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_l1=LogisticRegression()\n",
    "clf_l2=RidgeClassifier()\n",
    "clf_elasti=SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_lasso=GridSearchCV(clf_l1,param_grid=parameters_lasso,cv=kf)\n",
    "grid_search_ridge=GridSearchCV(clf_l2,param_grid=parameters_ridge,cv=kf)\n",
    "grid_search_elasticnet=GridSearchCV(clf_elasti,param_grid=parameters_elastic,cv=kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_lasso.fit(X_train,survived_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_lasso.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_lasso.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_ridge.fit(X_train,survived_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_ridge.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_elasticnet.fit(X_train,survived_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_elasticnet.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_elasticnet.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_elasti_new=SGDClassifier(alpha=0.0041320124001153384,l1_ratio=0.59948425031894093,loss='log',penalty='elasticnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_elasti_new.fit(X_train,survived_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=test.drop(['Ticket','Cabin','Name','Title','Surname'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=standar_scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survived_prediction=clf_elasti_new.predict(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission=pd.DataFrame({\"PassengerID\":test['PassengerId'],\"Survived\":survived_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('logistic_with_regularization.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
